#!/usr/bin/env python3
# coding: utf-8

# -*-mode: python-*-
from datetime import datetime, timezone, timedelta
from pprint import pprint
from termcolor import colored
from requests.exceptions import HTTPError
import argparse
import csv
import dateutil.parser
import fileinput
import filelock
import glob
import gzip
import itertools
import json
import logging
import os
import os.path
import pymongo
import re
import requests
import sys
import urllib.parse

__version__ = '0.3.0'

LOCKFILE_PATH = "/tmp/archive-processor.lock"

invocation_id = "ghc-{}-{}.log".format(
    os.uname().nodename,
    datetime.now().strftime('%Y%m%d-%H%M%S'))

# Setup logging
def make_logger(file_dir='.'):
    logger = logging.getLogger('processor[{}]'.format(os.uname().nodename))
    logging.Formatter.default_msec_format = '%s.%03d'
    log_formatter = logging.Formatter(
        fmt='%(asctime)s:%(name)s:%(levelname)s:%(message)s')
    file_log_handler = logging.FileHandler(os.path.join(file_dir,
                                                        invocation_id))
    console_log_handler = logging.StreamHandler()
    file_log_handler.setFormatter(log_formatter)
    console_log_handler.setFormatter(log_formatter)
    logger.addHandler(file_log_handler)
    logger.addHandler(console_log_handler)
    logger.setLevel(logging.INFO)
    return logger

log = make_logger(os.getenv('GHC_LOG_PATH'))

class EventLoader(object):
    """Loads transformed .json.gz files into a database (must be subclassed)
    """

    def __init__(self, transformed_path, loaded_path):
        """Construct new EventLoader
        Both parameter paths should be absolute, and must be on the same device
        as equality is compared using inodes.
        """
        self.transformed_path = transformed_path
        self.loaded_path = loaded_path

    def _find_loaded_inodes(self):
        loaded_files = glob.glob(os.path.join(self.loaded_path, '*.json.gz'))
        log.debug("found {} previously loaded files".format(len(loaded_files)))
        return set([os.stat(cf).st_ino for cf in loaded_files])

    def _find_sources(self):
        sources = glob.glob(os.path.join(self.transformed_path,
                                         '*.json.gz'))
        log.debug("found {} potential sources".format(len(sources)))
        sources = sorted([f for f in sources
                          if os.stat(f).st_ino
                          not in self._loaded_inodes])
        log.info("found {} sources needed loading".format(len(sources)))
        return sources


    def load(self):
        log.info("loading events from [{}]; saving progress to [{}]".format(
            self.transformed_path, self.loaded_path
        ))

        self._loaded_inodes = self._find_loaded_inodes()
        self._sources = self._find_sources()

        self._start_time = datetime.now()
        for source in self._sources:
            log.info("loading events from [{}]".format(source))
            self.load_file(source)
            self.mark_loaded(source)

    def _log_stats(self, record_count, start_time, done=False, source_path=None):
        duration = (datetime.now() - start_time).total_seconds()
        filename = os.path.basename(source_path)
        verb = { False: "loading in progress", True: "loaded" }[done]
        from_str = ""
        if source_path: from_str = " from [{}]".format(source_path)
        log.info("{}: {} records in {:.2f} seconds ({:.2f} r/s){}".format(
            verb, record_count, duration, record_count/duration, from_str
        ))


    def load_file(self, events_file):
        """Load a single events file into database
        """
        raise "EventLoader is abstract; I don't know how to load an events file"


    def mark_loaded(self, events_file):
        """Create link in "loaded" directory to file that was just loaded
        It's okay if this doesn't get written; duplicates won't make it into the DB
        because of the unique index on `_event_id`
        """
        basename = os.path.basename(events_file)
        link_path = os.path.join(self.loaded_path, basename)
        log.info("marking [{}] as loaded in [{}]".format(basename, link_path))
        os.link(events_file, link_path)

class MongoDBEventLoader(EventLoader):
    """Loads transformed .json.gz files into MongoDB
    This class does not load files atomically. However it will not load duplicate
    events due to a unique index on `_event_id`. If an error is encountered with
    this index, it will be ignored for this reason.
    """

    def __init__(self, transformed_path, loaded_path):
        super().__init__(transformed_path, loaded_path)
        self.db = pymongo.MongoClient().contributions
        log.info("connected to MongoDB")

    def load_file(self, events_file):
        """Load a single events file into MongoDB
        """
        log.info("going to load events from [{}]".format(events_file))
        gzip_start_time = datetime.now()
        with gzip.open(events_file, 'rt') as source:
            records = (json.loads(r) for r in source)
            log.info("loading records from {}".format(events_file))
            if not bool(records):
                log.error("will not bulk load empty file. look into this.")
                return
            load_start_time = datetime.now()
            count = 0

            while True:
                try:
                    # 1_000 at a time
                    records_chunk = list(itertools.islice(records, 1000))
                    if not records_chunk: break
                    count += len(records_chunk)
                    self.db.contributions.insert(records_chunk,
                                                 continue_on_error=True)
                except pymongo.errors.DuplicateKeyError as e:
                    log.debug("tried to load duplicate key(s) ({})".format(e))

            self._log_stats(len(records), load_start_time, True, events_file)

class GithubArchiveFetcher(object):
    """Retrieves needed .json.gz files from githubarchive.org
    """

    START = datetime(2015, 1, 1, 0)
    TIME_FMT = '%Y-%m-%d-%-H'
    BUFFER_SIZE = 1024 * 1024
    URL_TEMPLATE = "http://data.githubarchive.org/{}"

    def __init__(self, events_directory):
        if not events_directory:
            raise RuntimeError('no events directory specified')
        self.events_directory = events_directory

    def fetch(self):
        t = self.START
        # All the files should be in the GitHub Archive that are more than two hours old
        expect_files_until = datetime.utcnow() - timedelta(hours=2)
        while t <= datetime.utcnow():
            filename = "{}.json.gz".format(t.strftime(self.TIME_FMT))
            local_filename = os.path.join(self.events_directory, filename)
            log.debug("considering {}".format(local_filename))
            if not os.path.isfile(local_filename):
                try:
                    sz = self.fetch_file(filename, local_filename)
                    if sz: log.info('{}: got {:.2f} MB'.format(filename, sz/(1024*1024)))
                    else: log.info('download returned nothing')
                except HTTPError as e:
                    # It's fine if we miss the last 2 hours
                    # the file probably just isn't uploaded yet
                    if t >= expect_files_until:
                        msg = "encountered expected error for recent data file: {}"
                        log.info(msg.format(e))
                        break
                    else: raise e
            t += timedelta(hours=1)

    def fetch_file(self, filename, sink_path):
        """Retrieve one .json.gz file from GitHub Archive
        """
        url = self.URL_TEMPLATE.format(filename)
        log.info("fetching {} from {}".format(filename, url))
        if os.path.isfile(sink_path):
            raise RuntimeError('will not overwrite file already present')

        source = requests.get(url, stream=True)
        # Caught by fetch. 404 is expected for the last couple of hours' data.
        source.raise_for_status()
        sz = 0
        with open(sink_path, 'wb') as sink:
            for chunk in source.iter_content(self.BUFFER_SIZE):
                sz += len(chunk)
                sink.write(chunk)
            last_modified = source.headers['Last-Modified']
            last_modified = datetime.strptime(last_modified,
                                              '%a, %d %b %Y %H:%M:%S %Z')
            last_modified = last_modified.timestamp()
            os.utime(sink_path, (last_modified, last_modified))
            return sz

class ValidationError(RuntimeError):
    """Indicates that an Event was transformed but the result looks invalid.
    Suggests that the transformer is broken.
    """
    pass

class InvalidRecordError(RuntimeError):
    """Indicates that the event being transformed contains errors.
    Suggests that the input data is wrong; no fix necessary for transformer.
    """
    pass

class Event(object):
    types = dict()

    def __init__(self, raw_event, source_fmt):
        makers = {
            'event': (self.make_common_from_event, self.make_from_event),
            'timeline': (self.make_common_from_timeline, self.make_from_timeline)
        }
        maker = makers.get(source_fmt, None)
        if maker is None:
            raise "source_fmt should either be 'event' (json) or 'timeline' (csv)"
        for m in maker:
            m(raw_event)

    def make_common_from_event(self, raw_event):
        self.type = raw_event['type']
        self._event_id = raw_event['id']
        self.created_at = dateutil.parser.parse(raw_event['created_at']).isoformat()

        if 'actor' in raw_event:
            self.user = raw_event['actor']['login']
            self._user_lower = self.user.lower()
        if 'repo' in raw_event:
            self.repo = raw_event['repo']['name']
            self._repo_lower = self.repo.lower()
        if 'org' in raw_event:
            self.org = raw_event['org']['login']
            self._org_lower = self.org.lower()

    def make_common_from_timeline(self, raw_event):
        self.type = raw_event['type']
        created_at = datetime.strptime(raw_event['created_at'],
                                       '%Y-%m-%d %H:%M:%S')
        created_at = created_at.replace(tzinfo=timezone.utc)
        self.created_at = created_at.isoformat()
        self.user = raw_event['actor']
        self._user_lower = self.user.lower()
        self.org = raw_event['repository_organization']
        self._org_lower = self.org.lower()

        # At different points, "repository_name" means either:
        # owner/project or project (or it's just messed up)
        if '/' in raw_event['repository_name'] and raw_event['repository_name'] != '/':
            self.repo = raw_event['repository_name']
        elif raw_event['repository_owner'] and raw_event['repository_name']:
            self.repo = "{}/{}".format(raw_event['repository_owner'],
                                       raw_event['repository_name'])
        elif raw_event['url']:
            # Seriously? Yes, this (and others) otherwise lack the repo name:
            # SELECT * FROM [githubarchive:year.2014]
            # where payload_head='8824ed4d86f587a2a556248d9abfac790a1cbd3f'
            #                    ^^ this is a PushEvent
            u = urllib.parse.urlparse(raw_event['url'])
            frags = u.path.split('/')
            self.repo = '/'.join(frags[1:3])
        else:
            raise InvalidRecordError("timeline event does not contain repo name")

        self._repo_lower = self.repo.lower()


    def make_from_event(self, raw_event):
        """
        From new API, natively JSON (on/after 2015-01-01)
        """
        raise "{} must override make_from_event".format(self.__class__.__name__)

    def make_from_timeline(self, row):
        """
        From old API (before 2015-01-01)
        """
        raise "{} must override make_from_timeline".format(self.__class__.__name__)

    def _validate(self):
        raise "{} must override _validate".format(self.__class__.__name__)

    def _explode(self, reason):
        raise ValidationError(reason)

    def validate(self):
        """
        Raises ValidationError if something not Kosher is detected
        """
        if not re.match('^[\w\-\.]+/[\w\-\.]+$', self.repo):
            self._explode("repository [{}] doesn't match [owner/project]".format(self.repo))
        self._validate() # child validations

    @classmethod
    def register_event(cls, event_cls):
        cls.types[event_cls.__name__] = event_cls
        return event_cls

    @classmethod
    def from_raw(cls, raw_event, source_fmt):
        """
        Factory method to create appropriate event instance from raw JSON data
        """
        subclass = cls.types.get(raw_event['type'], None)
        if subclass is None:
            return None
        return subclass(raw_event, source_fmt)


register_event = Event.register_event

@register_event
class GollumEvent(Event):
    def make_from_event(self, d):
        self.pages = d['payload']['pages']

    def make_from_timeline(self, d):
        self.pages = [{
            'html_url': d["payload_page_html_url"],
            'summary': d["payload_page_summary"],
            'page_name': d["payload_page_page_name"],
            'action': d["payload_page_action"],
            'title': d["payload_page_title"],
            'sha': d["payload_page_sha"]
        }]

    def _validate(self):
        pass

@register_event
class IssuesEvent(Event):
    def make_from_event(self, d):
        self.action = d['payload']['action']
        issue = d['payload']['issue']
        self.html_url = issue['html_url']
        self.issue_number = issue['number']

    def make_from_timeline(self, d):
        self.action = d['payload_action']
        self.html_url = d['url']
        try:
            self.issue_number = int(d['payload_number'])
        except:
            self.issue_number = None

    def _validate(self):
        pass

@register_event
class PushEvent(Event):
    def make_from_event(self, d):
        p = d['payload']
        self.ref = p['ref']
        self.head = p['head']
        self.before = p['before']
        self.commit_count = p['size']
        self.distinct_commit_count = p['distinct_size']

    def make_from_timeline(self, d):
        # The "commit" here seems to be a random commit in the push.
        # Only one is listed, and it's not the first nor the last. ಠ_๏
        self.ref = d['payload_ref']
        self.head = d['payload_head']
        self.before = None
        self.commit_count = None
        self.distinct_count = None

    def _validate(self):
        pass

@register_event
class CommitCommentEvent(Event):
    def make_from_event(self, raw_event):
        comment = raw_event['payload']['comment']
        self.path = comment['path'] # to file, if applicable
        self.body = comment['body']
        self.comment_id = comment['id']
        self.commit_id = comment['commit_id']
        self.html_url = comment['html_url']

    def _url_from_timeline(self):
        template = "https://github.com/{e.repo}/commit/{e.commit_id}#commitcomment-{e.comment_id}"
        return template.format(e=self)

    def make_from_timeline(self, d):
        self.path = d['payload_comment_path']
        self.body = d['payload_comment_body']
        self.comment_id = d['payload_comment_id']
        self.commit_id = d['payload_commit_id']
        self.html_url = self._url_from_timeline()

    def _validate(self):
        pass


@register_event
class ReleaseEvent(Event):
    def make_from_event(self, raw_event):
        self.action = raw_event['payload']['action']
        r = raw_event['payload']['release']
        self.tag_name = r['tag_name']
        self.download_url = r['html_url']

    def make_from_timeline(self, raw_event):
        self.action = raw_event['payload_action']
        self.download_url = urllib.parse.urldefrag(raw_event['url']).url
        self.tag_name = os.path.basename(self.download_url)

    def _validate(self):
        pass

@register_event
class PublicEvent(Event):
    def make_from_event(self, d):
        pass

    def make_from_timeline(self, d):
        pass

    def _validate(self):
        pass

@register_event
class MemberEvent(Event):
    def make_from_event(self, raw_event):
        p = raw_event['payload']
        self.target_user = p['member']['login']
        self.target_action = p['action']

    def make_from_timeline(self, raw_event):
        self.target_user = raw_event['payload_member_login']
        self.target_action = raw_event['payload_action']

    def _validate(self):
        pass

@register_event
class IssueCommentEvent(Event):
    def make_from_event(self, raw_event):
        p = raw_event['payload']
        self.action = p['action']
        self.issue_number = p['issue']['number']
        self.issue_url = p['issue']['html_url']
        self.comment_url = p['comment']['html_url']
        self.comment_id = p['comment']['id']
        self.comment_body = p['comment']['body']


    def make_from_timeline(self, raw_event):
        self.issue_url = urllib.parse.urldefrag(raw_event['url']).url
        self.action=raw_event.get('payload_action', '') # closed, open, created, null

        # this timeline is so awful. ಠ_ಠ
        try:
            self.issue_number = int(os.path.basename(self.issue_url))
        except:
            self.issue_number = ''

        self.comment_url = raw_event['url']
        self.comment_id = None
        if raw_event['payload_commit_id']:
            self.comment_id = int(raw_event['payload_comment_id'])
        self.comment_body = raw_event['payload_comment_body']

    def _validate(self):
        pass

class EventTransformer(object):
    def __init__(self, source, destination):
        self.source = source
        self.write_event = destination
        self.total_record_count = 0
        self.count = 0 # success
        self.invalid_record_count = 0
        self.transform_error_count = 0

    def _make_stats(self):
        duration = (datetime.now() - self.start_time).total_seconds()
        return (self.total_record_count,
                duration,
                self.total_record_count / duration,
                self.count,
                self.invalid_record_count,
                self.transform_error_count)

    def transform_events(self):
        self.start_time = datetime.now()

        for raw_event, event_source in self.source:
            event = self.transform_event(raw_event, event_source)

            if self.total_record_count > 0 and self.total_record_count % 20000 == 0:
                duration = (datetime.now() - self.start_time).total_seconds()
                log.info("transforming: {} records in {:.2f} seconds ({:.2f} r/s): {}s {}i {}e".format(
                    *self._make_stats()))
            if event:
                self.write_event(event)
                self.count += 1

            self.total_record_count += 1

        stats = self._make_stats()
        log.info(colored("transforming: {} records in {:.2f} seconds ({:.2f} r/s): {}s {}i {}e".format(
            *stats), 'green'))
        log.info(colored("invalid records: {}".format(self.invalid_record_count),
                      'yellow'))
        log.info(colored("transformation errors: {}".format(self.transform_error_count),
                      'red'))


    def transform_event(self, raw_event, source_fmt):
        try:
            event = Event.from_raw(raw_event, source_fmt)
        except InvalidRecordError as e:
            # This is a known invalid record; nothing we can do.
            self.invalid_record_count += 1
            return None

        if event:
            try:
                event.validate()
            except ValidationError as e:
                # This is very bad.
                msg = "{} ({}):\n\t{}\n\t{}".format(
                    colored('Invalid Event', 'red', attrs=['bold']),
                    colored(str(e), 'red'),
                    json.dumps(event.__dict__).strip(),
                    json.dumps(raw_event).strip(),
                )
                log.error(msg)
                self.transform_error_count += 1
                return None

            return event

class EventSource(object):
    def __init__(self, path, file_predicate, file_change_subscriber=None):
        self.path = path
        self.file_predicate = file_predicate
        self.notify_file_change = file_change_subscriber

    def _filenames(self, pattern):
        files = glob.iglob(os.path.join(self.path, pattern))
        return (f for f in files
                if self.file_predicate(f))

    def _open_file(self, path, mode):
        log.debug("transforming file: {}".format(path))
        if self.notify_file_change:
            self.notify_file_change(path)
        return gzip.open(path,'rt')

class EventEventSource(EventSource):
    """
    Iterate over every event from Event API
    The class name seems idiotic, but the Event data can either come from the
    "Event API" or the "Timeline API" depending on the year.
    """

    def __iter__(self):
        # Iterator for lines of these source files
        lines = fileinput.FileInput(files=self._filenames('*.json.gz'),
                                    openhook=self._open_file)

        events = (json.loads(line) for line in lines)
        return ((event, 'event') for event in events)

class TimelineEventSource(EventSource):
    """
    Iterate over every event from Timeline API (2011-2014)
    """

    def __iter__(self):
        lines = fileinput.FileInput(files=self._filenames('*.csv.gz'),
                                    openhook=self._open_file)
        reader = csv.DictReader(lines)
        return ((line, 'timeline') for line in reader)

class EventSink(object):
    def __init__(self, destination_dir=None):
        self.out_path = '<STDOUT>'
        self.out_stream = sys.stdout
        self.destination_dir = destination_dir

    def already_transformed(self, input_path):
        if not self.destination_dir: return False

        sink_path = self.filename_transform(input_path)
        return os.path.exists(sink_path) and os.stat(sink_path).st_size > 0

    def filename_transform(self, input_path):
        if not self.destination_dir: return None

        input_path = os.path.basename(input_path)
        path, path_ext = os.path.splitext(input_path)
        return os.path.join(
            self.destination_dir,
            ''.join([path + '-processed.json', path_ext]))


    def notify_file_change(self, new_path):
        """Rotate our destination file
        """
        if self.destination_dir is None:
            # No place to put file anyway; always go to STDOUT
            return

        new_path = self.filename_transform(new_path)

        log.debug("eventsink: changing file: {} -> {}".format(
            self.out_path, new_path
        ))
        if self.out_stream and self.out_stream != sys.stdout:
            self.out_stream.close()
        self.out_path = new_path
        self.out_stream = open(new_path, 'wb')

        path, path_ext = os.path.splitext(new_path)
        if '.gz' in path_ext:
            log.debug('eventsink: will gzip output')
            self.out_stream = gzip.open(self.out_stream,'wb')

    def write(self, event):
        output = json.dumps(event.__dict__).strip() + '\n'
        self.out_stream.write(output.encode())

def fetch(args):
    fetcher = GithubArchiveFetcher(args.events_path)
    fetcher.fetch()

def transform(args):
    sink = EventSink(args.transformed_path)
    file_predicate = lambda f: not sink.already_transformed(f)

    sources = []
    if args.events_path:
        source = EventEventSource(args.events_path,
                                  file_predicate,
                                  sink.notify_file_change)
        sources.append(source)
    if args.timeline_path:
        source = TimelineEventSource(args.timeline_path,
                                     file_predicate,
                                     sink.notify_file_change)
        sources.append(source)
    source = itertools.chain(*sources)

    transformer = EventTransformer(source, sink.write)
    transformer.transform_events()

def load(args):
    loader = MongoDBEventLoader(args.transformed_path, args.loaded_path)
    loader.load()

def process(args):
    fetch(args)
    transform(args)
    load(args)

if __name__=='__main__':

    parser = argparse.ArgumentParser(description='Process GitHub archive files')

    parser.add_argument('--events-path',
                        default=os.getenv('GHC_EVENTS_PATH'))
    parser.add_argument('--timeline-path',
                        default=os.getenv('GHC_TIMELINE_PATH'))
    parser.add_argument('--transformed-path',
                        default=os.getenv('GHC_TRANSFORMED_PATH'))
    parser.add_argument('--loaded-path',
                        default=os.getenv('GHC_LOADED_PATH'))

    subparsers = parser.add_subparsers(dest='mode')

    process_parser = subparsers.add_parser('process')
    process_parser.set_defaults(func=process)

    fetch_parser = subparsers.add_parser('fetch')
    fetch_parser.set_defaults(func=fetch)

    transform_parser = subparsers.add_parser('transform')
    transform_parser.set_defaults(func=transform)

    load_parser = subparsers.add_parser('load')
    load_parser.set_defaults(func=load)

    args = parser.parse_args()

    if args.mode is None:
        parser.print_help()
        sys.exit(1)

    lock = filelock.FileLock(LOCKFILE_PATH)
    with lock:
        try:
            args.func(args)
        except Exception as e:
            log.error("unhandled fatal error: {}".format(e))
            raise e
